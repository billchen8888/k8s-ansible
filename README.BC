# I add one script file to prepare the data.

We put the IPs of the controlbox as the first tone, then thr cluster IPs in USER data
then add private key to root/.ssh/id_rsa

becises this script, I also modified the group_vars/all   the docker_repo to use docker, not aliyun
                     add the ignor_error for firewalld service stop in roles/init/stop_firewalld.yml permantently

run   BC_prepare.sh  as root

then run download_binary.sh and then
ansible-playbook -i inventory/hosts site.yml


KNOWN issues to the original package:
1)  the roles/master/tasks/make_master_labels_and_taints.yml label/taint master nodes would fail as in the inventory/hosts I put the IPs. THe IP is not the node name k8s is aware. 
    k8s is aware of the actual hostname. So I changed the code to use {{ ansible_hostname }}. In order to use {{ ansible_hostname }}, I need to gather_facts in the top level 
    yml file (the one spcified for ansible-playbook command), and add  setup in the roles/master/task/main.yml  or in the child yml  file (make_master_labels_and_taints.yml

2) even after I did step1, and let the ansible sleep 180 (now 300) seonds to let all nodes ready, sonehow make_master_labels_and_taints.yml  still complains the nodes are not ready.

3ï¼‰80% of time that one master node is not good, and a lot of component are not even installed. After many testing and debug, the issue was many the steps in role/master/tasks/ with connection: local would need 
   run_once: true.  The issue is on the localbox, kubectl commands might create a temporary lock file. If the ansible runs too fast, then certain kubectl (sepcially the one 
   for kubeconfig generationcause problem.
   
 4) I fixed 3). now most of pods in CrashLoopBackOff or CreatingCaontainer state
