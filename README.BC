# we need certains security group policy for the nodes as they need to communicate each other.
#    for example, the etcd nodes need to check each other, otherwise  systemctl start etcd   will hang there forever.
# we also need to pay attention to the component version specified in group_vars/all, as the repo might not have the old 
#    version any more, such as centos8 repo doesn't have docker-ce 1.19.3 etc
#
# note: this works only on centos7, not centos8 yet.
# the issue in centos8: 
#  a)  iptables package is not pre-installed, so roles/init/tasks/stop_firewall.yml  will fail on the iptables command
#      we can edit roles/init/tasks/main.yml  and move install_some_pkgs.yml  to the first, so iptables will be present before we use iptables
#  b)  roles/init/tasks/sysctl.yml  will also fail on the line fs.may_detach_mounts=1 of 99-k8s.conf 
# I add one script file to prepare the data.

We put the IPs of the controlbox as the first tone, then thr cluster IPs in USER data
then add private key to root/.ssh/id_rsa

becides this script, I also modified the group_vars/all   the docker_repo to use docker, not aliyun
                     add the ignor_error for firewalld service stop in roles/init/stop_firewalld.yml permantently

# How to use:
add /root/.ssh/id_rsa  key
modify the IPs in USERDATA. The first one is controlbox; then 3 master and 2 worker
run   BC_prepare.sh  as root
then run download_binary.sh and then
ansible-playbook -i inventory/hosts site.yml


KNOWN issues to the original package:
1)  the roles/master/tasks/make_master_labels_and_taints.yml label/taint master nodes would fail as in the inventory/hosts I put the IPs. THe IP is not the node name k8s is aware. 
    k8s only knows the actual hostname. So I changed the code to use {{ ansible_hostname }}. In order to use {{ ansible_hostname }}, I need to gather_facts in the top level 
    yml file (the one spcified for ansible-playbook command), and add  setup in the roles/master/task/main.yml  or in the child yml  file (make_master_labels_and_taints.yml

2) even after I did step1, and let the ansible sleep 180 (now 300) seonds to let all nodes ready, sonehow make_master_labels_and_taints.yml  still complains the nodes are not ready.

3ï¼‰80% of time that one master node is not good, and a lot of component are not even installed. After many testing and debug, the issue was many the steps in role/master/tasks/ with connection: local would need 
   run_once: true.  The issue is on the localbox, kubectl commands might create a temporary lock file. If the ansible runs too fast, then certain kubectl (sepcially the one 
   for kubeconfig generationcause problem.
   
 4) I fixed 3). Now most of pods in CrashLoopBackOff or CreatingCaontainer state. The issue is aws uses eth0 for the NIC, but the original script uses ens33 (which is set NETWORK_ADAPTER_NAME in group_vars/all)
 
# issues I ran into: (still happenning, need to sort out why)
 1) I get the following error twice in 3 deployment: the step "kubectl apply -f coredns.yaml" in coredns/tasks/main.yml failed with the following error:
 
    Error from server: error when retrieving current configuration of:
    Resource: "/v1, Resource=serviceaccounts", GroupVersionKind: "/v1, Kind=ServiceAccount"
    Name: "coredns", Namespace: "kube-system"
    from server for: "coredns.yaml": etcdserver: request timed out"
    
    but most of the roles were created (except one:  serviceaccount/coredns), and no pods for coredns.
    clusterrole.rbac.authorization.k8s.io/system:coredns created
    clusterrolebinding.rbac.authorization.k8s.io/system:coredns created
    configmap/coredns created
    deployment.apps/coredns created
    service/kube-dns created"
   
    on one master node, I maunally run:  cd /opt/yamls;  kubectl apply -f coredns.yaml
    sometime later, the two coredns pods are up running, and all the DESIRED and READY or CURRENT numbers match in kubectl get all -A
    
    this issue didn' occurr in the other deployment (exatchly same code)
    
 
# The actual change I made to the main ansible:
 1) in make_master_labels_and_taints.yml  to use {{ ansible_hostname }}, to do this, I need to specifically use setup modeul to gather facts
 2) add run_once for the place with connection: local, as no need to run exactly same commands on the controlbox multiple times (and each time with exact same generated conf file). The kube lock file causes issues
 3) changed the network_adapter_name to be eth0 in group_vars/all
 4) changed the aliyuns repo and registry as at least the registery is not reachable



The final deployment result with this script: 
1 controlbox (also nginx for lod balancer); 3 master; 2 worker

total 9 pods:
5 calico-node  (every master and worker host)
1 calico-kube-controller (only on 1 master host)
2 coredns  (1 master and 1 worker)
1 metric   (only on 1 worker)

#Things to do:
  move the make_master_labels_and_taints after the calico. The nodes would not show up before kubectl apply -f calico.yml
